##
# Dockerfile-boom-gpu
# CUDA-enabled build & runtime for boom Rust binaries using ONNX Runtime (via ort crate with "cuda" feature).
# Assumptions:
#  - Cargo.toml already enables `ort` with feature cuda on Linux (present in project).
#  - Host machine running containers has NVIDIA driver + nvidia-container-toolkit installed.
#  - This image targets linux/amd64 with NVIDIA GPUs. (Apple GPUs not exposed inside Linux containers.)
#
# Build:
#   docker build -f Dockerfile-boom-gpu -t boom/boom-benchmarking:gpu --target runtime-gpu .
# Run (single binary example):
#   docker run --rm --gpus=all boom/boom-benchmarking:gpu /app/scheduler ztf
#
# If you only have CPU, keep using original Dockerfile-boom or allow fallback by setting
#   ORT_DONT_FAIL_ON_PROVIDER_NOT_FOUND=1 (already set below) so CUDA absence doesn't crash.
##

ARG RUST_VERSION=1.87.0

############################
# GPU BUILDER STAGE        #
############################
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder-gpu
ARG RUST_VERSION
WORKDIR /app

ENV DEBIAN_FRONTEND=noninteractive \
    PATH=/root/.cargo/bin:$PATH \
    RUSTFLAGS="-C target-cpu=native"

# Build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      curl git build-essential pkg-config perl make \
      libhdf5-dev libsasl2-dev libssl-dev ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Install Rust toolchain explicitly (the CUDA base image lacks Rust)
RUN curl https://sh.rustup.rs -sSf | sh -s -- -y --profile minimal --default-toolchain ${RUST_VERSION} && \
    rustc --version && cargo --version

# Pre-copy manifests for layer caching
COPY boom/Cargo.toml boom/Cargo.lock ./

# (Optional) dependency fetch to speed later rebuilds
RUN mkdir src && echo "fn main(){}" > src/main.rs && cargo build --release || true && rm -rf src

# Copy sources
COPY boom/src ./src
COPY boom/api ./api
# Models (needed at runtime; include now to COPY once later)
COPY boom/data/models ./data/models

# Build all workspace binaries (release)
RUN cargo build --release --workspace

# Collect ONNX Runtime shared libraries (downloaded by ort crate under target)
# Previous attempt missed provider libs due to directory depth; search deeper.
RUN set -eux; \
    mkdir -p /opt/onnxruntime/lib; \
    # Find all relevant onnxruntime shared objects (core + providers) anywhere under target
    find target -type f -name 'libonnxruntime*.so*' -exec cp -v {} /opt/onnxruntime/lib/ \; || true; \
    echo "Copied ONNX Runtime libs:"; ls -1 /opt/onnxruntime/lib || true; \
    if [ ! -f /opt/onnxruntime/lib/libonnxruntime_providers_shared.so ]; then \
        echo 'WARNING: libonnxruntime_providers_shared.so not found. The CUDA EP may fail to load.'; \
        find target -type f -name 'libonnxruntime_providers_shared.so' || true; \
    fi

############################
# GPU RUNTIME STAGE        #
############################
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS runtime-gpu
WORKDIR /app

# Minimal runtime deps (libsasl2 for kafka auth, certs for TLS)
RUN apt-get update && apt-get install -y --no-install-recommends \
      libsasl2-2 ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Copy binaries & models
COPY --from=builder-gpu /app/target/release/scheduler /app/scheduler
COPY --from=builder-gpu /app/target/release/kafka_consumer /app/kafka_consumer
COPY --from=builder-gpu /app/target/release/kafka_producer /app/kafka_producer
COPY --from=builder-gpu /app/data/models /app/data/models

# Copy ONNX Runtime libs (if present). If not found, container still works (CPU fallback)
COPY --from=builder-gpu /opt/onnxruntime/lib /opt/onnxruntime/lib

ENV LD_LIBRARY_PATH=/opt/onnxruntime/lib:${LD_LIBRARY_PATH} \
    RUST_LOG=info,ort=error \
    ORT_LOG_SEVERITY_LEVEL=3 \
    ORT_DONT_FAIL_ON_PROVIDER_NOT_FOUND=1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Default entrypoint (overridden by compose service commands)
CMD ["/app/scheduler", "ztf"]

# Notes:
# - To confirm GPU provider usage, add logging in Rust (print available providers) or run with:
#     docker run --rm --gpus=all boom/boom-benchmarking:gpu /app/scheduler ztf --list-providers
#   (Implement a flag in your binaries accordingly.)
# - If libonnxruntime*.so isn't found, ensure ort crate downloaded GPU build (feature "cuda" on linux) and adjust search depth.
