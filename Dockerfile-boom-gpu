##
# Dockerfile-boom-gpu
# CUDA-enabled build & runtime for boom Rust binaries using ONNX Runtime (via ort crate with "cuda" feature).
# Assumptions:
#  - Cargo.toml already enables `ort` with feature cuda on Linux (present in project).
#  - Host machine running containers has NVIDIA driver + nvidia-container-toolkit installed.
#  - This image targets linux/amd64 with NVIDIA GPUs. (Apple GPUs not exposed inside Linux containers.)
#
# Build:
#   docker build -f Dockerfile-boom-gpu -t boom/boom-benchmarking:gpu --target runtime-gpu .
# Run (single binary example):
#   docker run --rm --gpus=all boom/boom-benchmarking:gpu /app/scheduler ztf
#
# If you only have CPU, keep using original Dockerfile-boom or allow fallback by setting
#   ORT_DONT_FAIL_ON_PROVIDER_NOT_FOUND=1 (already set below) so CUDA absence doesn't crash.
##

ARG RUST_VERSION=1.87.0
ARG ORT_VERSION=1.19.0

############################
# ONNX Runtime (GPU) libs  #
############################
# Download official ORT GPU release to guarantee provider shared libs are present.
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS onnxruntime-gpu-libs
ARG ORT_VERSION
WORKDIR /opt
RUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates tar && \
    curl -L -o onnxruntime.tgz https://github.com/microsoft/onnxruntime/releases/download/v${ORT_VERSION}/onnxruntime-linux-x64-gpu-${ORT_VERSION}.tgz && \
    tar -xzf onnxruntime.tgz && mv onnxruntime-linux-x64-gpu-${ORT_VERSION} onnxruntime && \
    rm onnxruntime.tgz && rm -rf /var/lib/apt/lists/*
RUN ls -1 onnxruntime/lib | sort

############################
# GPU BUILDER STAGE        #
############################
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder-gpu
ARG RUST_VERSION
ARG ORT_VERSION
WORKDIR /app

ENV DEBIAN_FRONTEND=noninteractive \
    PATH=/root/.cargo/bin:$PATH \
    RUSTFLAGS="-C target-cpu=native"

# Build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      curl git build-essential pkg-config perl make \
      libhdf5-dev libsasl2-dev libssl-dev ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Install Rust toolchain explicitly (the CUDA base image lacks Rust)
RUN curl https://sh.rustup.rs -sSf | sh -s -- -y --profile minimal --default-toolchain ${RUST_VERSION} && \
    rustc --version && cargo --version

# Copy in official ORT libs (so build script can load-dynamic)
COPY --from=onnxruntime-gpu-libs /opt/onnxruntime /opt/onnxruntime
ENV ONNXRUNTIME_LIB_DIR=/opt/onnxruntime/lib \
    ONNXRUNTIME_INCLUDE_DIR=/opt/onnxruntime/include \
    LD_LIBRARY_PATH=/opt/onnxruntime/lib:${LD_LIBRARY_PATH}

# Pre-copy manifests for layer caching
COPY boom/Cargo.toml boom/Cargo.lock ./

# (Optional) dependency fetch to speed later rebuilds
RUN mkdir src && echo "fn main(){}" > src/main.rs && cargo build --release || true && rm -rf src

# Copy sources
COPY boom/src ./src
COPY boom/api ./api
# Models (needed at runtime; include now to COPY once later)
COPY boom/data/models ./data/models

# Build all workspace binaries (release)
RUN cargo build --release --workspace

# (Optional) capture any crate-downloaded variants (should be redundant now)
RUN set -eux; \
    find target -type f -name 'libonnxruntime*.so*' -exec bash -c 'dest=/opt/onnxruntime/lib/$(basename $0); [ -f "$dest" ] || cp -v "$0" /opt/onnxruntime/lib/' {} \; || true; \
    echo "Final ORT libs present:"; ls -1 /opt/onnxruntime/lib | sort; \
    test -f /opt/onnxruntime/lib/libonnxruntime_providers_shared.so

############################
# GPU RUNTIME STAGE        #
############################
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS runtime-gpu
WORKDIR /app

# Minimal runtime deps (libsasl2 for kafka auth, certs for TLS)
RUN apt-get update && apt-get install -y --no-install-recommends \
      libsasl2-2 ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Copy binaries & models
COPY --from=builder-gpu /app/target/release/scheduler /app/scheduler
COPY --from=builder-gpu /app/target/release/kafka_consumer /app/kafka_consumer
COPY --from=builder-gpu /app/target/release/kafka_producer /app/kafka_producer
COPY --from=builder-gpu /app/data/models /app/data/models

# Copy official ONNX Runtime libs (guaranteed provider shared lib)
COPY --from=onnxruntime-gpu-libs /opt/onnxruntime /opt/onnxruntime
# (Any crate-downloaded additional libs copied as well just in case)
COPY --from=builder-gpu /opt/onnxruntime/lib /opt/onnxruntime/lib

ENV LD_LIBRARY_PATH=/opt/onnxruntime/lib:${LD_LIBRARY_PATH} \
    RUST_LOG=info,ort=error \
    ORT_LOG_SEVERITY_LEVEL=3 \
    ORT_DONT_FAIL_ON_PROVIDER_NOT_FOUND=1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Default entrypoint (overridden by compose service commands)
CMD ["/app/scheduler", "ztf"]

# Notes:
# - To confirm GPU provider usage, add logging in Rust (print available providers) or run with:
#     docker run --rm --gpus=all boom/boom-benchmarking:gpu /app/scheduler ztf --list-providers
#   (Implement a flag in your binaries accordingly.)
# - If libonnxruntime*.so isn't found, ensure ort crate downloaded GPU build (feature "cuda" on linux) and adjust search depth.
